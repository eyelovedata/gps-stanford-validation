{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f941a-4192-48cf-8872-aae22f2f7828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model, regularizers, optimizers, metrics\n",
    "from random import randint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2feaa30-2739-4ac7-ac97-3cb646ed0c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8493bca-44fb-4f99-ac07-d130edbe3382",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74efcc4-1aed-400d-b3b9-d6f531cac996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_similarity_loss = keras.losses.CosineSimilarity(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "class CosineEmbeddingLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, reduction=tf.keras.losses.Reduction.AUTO, name=\"CosineEmbeddingLoss\"):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        similarity = -cosine_similarity_loss(y_true, y_pred)\n",
    "        return tf.reduce_mean(1. - similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d591e-12f6-4ade-8e73-3654b309148a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_autoencoder_model(data_shape,latent_size = 128, regularizer_term = 10e-6):\n",
    "    stacked_encoder = models.Sequential([\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1028, activation = 'selu',activity_regularizer=regularizers.l2(regularizer_term)),\n",
    "        layers.Dense(512, activation = 'selu',activity_regularizer=regularizers.l2(regularizer_term)),\n",
    "        layers.Dense(latent_size, activation = 'selu',activity_regularizer=regularizers.l2(regularizer_term)),\n",
    "    ])\n",
    "\n",
    "    stacked_decoder = models.Sequential([\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation = 'selu',activity_regularizer=regularizers.l2(regularizer_term)),\n",
    "        layers.Dense(1028, activation = 'selu',activity_regularizer=regularizers.l2(regularizer_term)),\n",
    "        layers.Dense(data_shape[1], activation = 'sigmoid',activity_regularizer=regularizers.l2(regularizer_term)),\n",
    "        layers.Reshape(data_shape[1:])\n",
    "    ])\n",
    "\n",
    "    stacked_autoencoder = models.Sequential([\n",
    "        stacked_encoder,\n",
    "        stacked_decoder\n",
    "    ])\n",
    "\n",
    "    stacked_autoencoder.build((None,data_shape[1]))\n",
    "    stacked_autoencoder.compile(optimizers.Adam(learning_rate = 0.001), loss=CosineEmbeddingLoss())\n",
    "    stacked_autoencoder.summary()\n",
    "    \n",
    "    return stacked_autoencoder, stacked_encoder, stacked_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacea20c-806d-4c46-ab4b-4a42df3e77f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    data = pd.read_csv(file).set_index('MRN')\n",
    "    data = data.astype('float')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95a289-114c-4723-8ac6-2aa66bc9bfab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf63e6-c3ee-4610-8b20-6245f7c1308f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file = 'processed_data/drugs_data.csv'\n",
    "output_file = 'processed_data/4_21_retrained_drugs_stack_autoencoder.csv'\n",
    "title = 'Drugs Features'\n",
    "\n",
    "regularizer_term = 10e-6\n",
    "latent_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27cd84-2fde-4ecd-8e97-b8d5ab39c5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = get_data(input_file)\n",
    "model, stacked_encoder, stacked_decoder = get_autoencoder_model(data.shape, latent_size = 128, \n",
    "                                                                    regularizer_term = 10e-6)\n",
    "\n",
    "#train\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
    "history = model.fit(data, data, epochs=300, batch_size=256, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb1d72-66fe-4f0f-883d-c7be80320710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(7,7))\n",
    "axs.plot(history.history['loss'])\n",
    "axs.title.set_text('Training Loss For ' + title)\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend(['Train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0eb1df-1087-4381-82f3-3408887effda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "batch_size = 32  # Reduce if still hitting memory limits\n",
    "num_samples = data.shape[0]\n",
    "features_list = []\n",
    "\n",
    "features_extractor_autoencoder = Model(inputs = stacked_encoder.inputs,\n",
    "                                      outputs = stacked_encoder.get_layer(index=3).output)\n",
    "\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    batch = data.values[i:i+batch_size]  # Slice the data into a batch\n",
    "    features_batch = features_extractor_autoencoder(batch)\n",
    "    features_list.append(features_batch.numpy())  # Convert to NumPy array and store\n",
    "\n",
    "# Concatenate all extracted features\n",
    "features = np.concatenate(features_list, axis=0)\n",
    "\n",
    "# Save\n",
    "features_df = pd.DataFrame(data = features, \n",
    "                    columns = [('drugs_embed' + str(i+1)) for i in range(features.shape[1])]) \n",
    "features_df.insert(0, 'MRN', data.index)\n",
    "features_df.to_csv(output_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da0b81-8b9c-4a9d-9437-322aaa21ee85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_extractor_autoencoder.save(\"models_4_21/drugs_autoencoder_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9f617-de4e-48b0-8264-c0fa60dc2076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c530b-83c1-465b-b1b9-60e6b83f2b70",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use All of Us Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd67367-9383-4ff7-970d-4eba7c2e86dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# import numpy as np\n",
    "# input_file = 'processed_data/drugs_data.csv'\n",
    "# output_file = 'processed_data/drugs_stack_autoencoder.csv'\n",
    "\n",
    "# data = get_data(input_file)\n",
    "# batch_size = 32  # Reduce if still hitting memory limits\n",
    "# num_samples = data.shape[0]\n",
    "# features_list = []\n",
    "\n",
    "# # It can be used to reconstruct the model identically.\n",
    "# reconstructed_model = keras.models.load_model(\"models/drugs_autoencoder_model_ALL_OF_US.keras\")\n",
    "\n",
    "# for i in range(0, num_samples, batch_size):\n",
    "#     batch = data.values[i:i+batch_size]  # Slice the data into a batch\n",
    "#     features_batch = reconstructed_model(batch)\n",
    "#     features_list.append(features_batch.numpy())  # Convert to NumPy array and store\n",
    "\n",
    "# # Concatenate all extracted features\n",
    "# features = np.concatenate(features_list, axis=0)\n",
    "\n",
    "# #save\n",
    "# features_df = pd.DataFrame(data = features, \n",
    "#                     columns = [('drugs_embed' + str(i+1)) for i in range(features.shape[1])]) \n",
    "# print(features_df.shape)\n",
    "# features_df.insert(0, 'MRN', data.index)\n",
    "# features_df.to_csv(output_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e850794-0b39-472c-82b8-6d58cea17f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a4d03-361a-4cff-8a30-8a60f482dbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7734dbc-fc64-4b99-a377-02ae4ddbf364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
