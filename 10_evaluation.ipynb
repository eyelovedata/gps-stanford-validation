{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f8f456-3bd6-485a-b282-199d803d093c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, balanced_accuracy_score, roc_curve, auc, precision_recall_curve, confusion_matrix\n",
    "import glob\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1da87-69d9-4c36-b39c-2f5c22aec4c6",
   "metadata": {},
   "source": [
    "## Metrics Tables - Standard Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2723713a-add4-46d3-9d74-7d6bfd348971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictionfile, outcome_col=\"outcome\", pred_col=\"Prediction\", pred_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for model predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing actual outcomes and predicted probabilities.\n",
    "    - outcome_col: Name of the column with true binary outcomes (0 or 1).\n",
    "    - pred_col: Name of the column with predicted probabilities.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame row with computed metrics.\n",
    "    \"\"\"\n",
    "    df_predictions=pd.read_csv(predictionfile)\n",
    "    cohort = pd.read_csv(\"processed_data/cohort.csv\")\n",
    "    df=pd.merge(cohort, df_predictions, on='MRN', how='inner')\n",
    "    \n",
    "    # Extract true labels and predicted probabilities\n",
    "    y_true = df[outcome_col]\n",
    "    y_scores = df[pred_col]\n",
    "\n",
    "    # Compute AUROC\n",
    "    auroc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    # Compute Precision-Recall curve and AUPRC\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    auprc = auc(recall, precision)\n",
    "\n",
    "    # Convert probabilities to binary predictions using 0.5 threshold\n",
    "    y_pred = (y_scores >= pred_threshold).astype(int)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Compute confusion matrix (TN, FP, FN, TP)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # Compute Sensitivity (Recall)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    # Compute Specificity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    # Compute Positive Predictive Value (PPV) - Precision\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "    # Compute Negative Predictive Value (NPV)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    # Compute F1 Score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Create a DataFrame row with results\n",
    "    results = pd.DataFrame([{\n",
    "        \"Model\": predictionfile.removeprefix(\"final_predictions_\"),\n",
    "        \"AUROC\": auroc,\n",
    "        \"AUPRC\": auprc,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"PPV (Precision)\": ppv,\n",
    "        \"NPV\": npv, \n",
    "        \"F1 Score\": f1\n",
    "    }])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81120d-99a0-425f-b94c-38291c79c008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_predictions=pd.read_csv('out/final_test_predictions_AoUencoder_StanfordFinetune-15-100pct.csv')\n",
    "\n",
    "cohort = pd.read_csv(\"processed_data/cohort.csv\")\n",
    "demo_df = pd.read_csv('processed_data/demo_not_1h_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54eab3-10d6-48de-9026-a86d239888ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(cohort, df_predictions, on='MRN', how='inner')\n",
    "df = df.merge(demo_df, left_on='MRN', right_on='MRN', how='left')\n",
    "df['race_consolidated'] = df['race'].replace({'aian': 'other', 'nhpi': 'asian'})\n",
    "df.loc[df['ethnicity'].str.lower() == 'hispanic', 'race_consolidated'] = 'hispanic'\n",
    "\n",
    "df['race_consolidated'] = df['race_consolidated'].str.capitalize()\n",
    "df['race_consolidated'] = df['race_consolidated'].apply(lambda x: 'Non-Hispanic ' + x.capitalize() if (x.lower() != 'hispanic' and x.lower() != 'other') else x.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db229be-75b8-4cd5-ae5b-1a3c35b1cf65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194fe8c7-9904-4b0b-beb7-ba84fa582f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b66ac-1c05-43a6-ab25-688531f644ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['race_consolidated'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ab186-60f9-486e-9676-8f6a9f62ddb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true = df[\"outcome\"]\n",
    "y_scores = df[\"Prediction\"]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Compute Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot ROC Curve\n",
    "ax[0, 0].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUROC = {roc_auc:.3f})')\n",
    "ax[0, 0].plot([0, 1], [0, 1], color='grey', linestyle='--')  # Random chance line\n",
    "ax[0, 0].set_xlim([0.0, 1.0])\n",
    "ax[0, 0].set_ylim([0.0, 1.05])\n",
    "ax[0, 0].set_xlabel('False Positive Rate', fontsize=20)\n",
    "ax[0, 0].set_ylabel('True Positive Rate', fontsize=20)\n",
    "ax[0, 0].set_title('ROC Curve', fontsize=24)\n",
    "ax[0, 0].legend(loc=\"lower right\", fontsize=16)\n",
    "ax[0, 0].grid()\n",
    "ax[0, 0].text(-0.1, 1.1, 'A', transform=ax[0, 0].transAxes, fontsize=24, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "ax[0, 1].plot(recall, precision, color='green', lw=2, label=f'PR curve (AUPRC = {pr_auc:.3f})')\n",
    "ax[0, 1].set_xlim([0.0, 1.0])\n",
    "ax[0, 1].set_ylim([0.0, 1.05])\n",
    "ax[0, 1].set_xlabel('Recall', fontsize=20)\n",
    "ax[0, 1].set_ylabel('Precision', fontsize=20)\n",
    "ax[0, 1].set_title('Precision-Recall Curve', fontsize=24)\n",
    "ax[0, 1].legend(loc=\"lower left\", fontsize=16)\n",
    "ax[0, 1].grid()\n",
    "ax[0, 1].text(-0.1, 1.1, 'B', transform=ax[0, 1].transAxes, fontsize=24, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "# ROC and Precision-Recall Curves stratified by race\n",
    "unique_races = df['race_consolidated'].unique()\n",
    "\n",
    "# Iterate over each unique race\n",
    "for race in unique_races:\n",
    "    race_subset = df[df['race_consolidated'] == race]\n",
    "    y_true = race_subset[\"outcome\"]\n",
    "    y_scores = race_subset[\"Prediction\"]\n",
    "\n",
    "    # ROC and AUROC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax[1, 0].plot(fpr, tpr, lw=2, label=f'{race} (AUROC = {roc_auc:.3f})')\n",
    "\n",
    "    # Precision-Recall and AUPRC\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    ax[1, 1].plot(recall, precision, lw=2, label=f'{race} (AUPRC = {pr_auc:.3f})')\n",
    "\n",
    "# Configure ROC plot\n",
    "ax[1, 0].plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "ax[1, 0].set_xlim([0.0, 1.0])\n",
    "ax[1, 0].set_ylim([0.0, 1.05])\n",
    "ax[1, 0].set_xlabel('False Positive Rate', fontsize=20)\n",
    "ax[1, 0].set_ylabel('True Positive Rate', fontsize=20)\n",
    "ax[1, 0].set_title('ROC Curves by Race/Ethnicity', fontsize=24)\n",
    "ax[1, 0].legend(loc=\"lower right\", fontsize=16)\n",
    "ax[1, 0].grid()\n",
    "ax[1, 0].text(-0.1, 1.1, 'C', transform=ax[1, 0].transAxes, fontsize=24, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "# Configure Precision-Recall plot\n",
    "ax[1, 1].set_xlim([0.0, 1.0])\n",
    "ax[1, 1].set_ylim([0.0, 1.05])\n",
    "ax[1, 1].set_xlabel('Recall', fontsize=20)\n",
    "ax[1, 1].set_ylabel('Precision', fontsize=20)\n",
    "ax[1, 1].set_title('Precision-Recall Curves by Race/Ethnicity', fontsize=24)\n",
    "ax[1, 1].legend(loc=\"lower left\", fontsize=16)\n",
    "ax[1, 1].grid()\n",
    "ax[1, 1].text(-0.1, 1.1, 'D', transform=ax[1, 1].transAxes, fontsize=24, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/rocs_and_prs.tiff', bbox_inches='tight', format='tiff')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb3c68-9ac4-4cbd-8040-f04465522ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example for fixed number of trainable layers\n",
    "prediction_files = glob.glob(\"out/final_test_predictions_AoUencoder_StanfordFinetune-15-*pct.csv\")\n",
    "\n",
    "plot_multiple_roc_pr_curves(prediction_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46311dde-eaef-499c-a39f-c4a74825e71c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is a baseline set of metrics assuming a 0.5 threshold for prediction. \n",
    "\n",
    "prediction_files = glob.glob(\"out/final_test_predictions_*.csv\")\n",
    "# Uncomment this line instead to look at the retrained autoencoder predictions\n",
    "# prediction_files = glob.glob(\"out/retrain_autoencoders_final_test_predictions_*.csv\")\n",
    "\n",
    "# Compute metrics for all models\n",
    "all_metrics = pd.concat([evaluate_predictions(file).assign(Model=file) for file in prediction_files])\n",
    "all_metrics.to_csv(\"pre_threshold_test_metrics_5_8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6237c-b05b-4081-8ee4-85bc5b49b5b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Looking for threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b670c-bfa9-4c47-b573-28407d45b63b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_threshold(predictionfile, outcome_col=\"outcome\", pred_col=\"Prediction\", threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute classification metrics at a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth binary labels (0 or 1).\n",
    "    - y_scores: Predicted probabilities.\n",
    "    - threshold: Decision threshold for classification.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of computed metrics.\n",
    "    \"\"\"\n",
    "    df_predictions=pd.read_csv(predictionfile)\n",
    "    cohort = pd.read_csv(\"processed_data/cohort.csv\")\n",
    "    df=pd.merge(cohort, df_predictions, on='MRN', how='inner')\n",
    "    \n",
    "    # Extract true labels and predicted probabilities\n",
    "    y_true = df[outcome_col]\n",
    "    y_scores = df[pred_col]\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    # Compute standard metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Compute confusion matrix (TN, FP, FN, TP)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # Compute Specificity (True Negative Rate)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    # Compute Negative Predictive Value (NPV)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "\n",
    "    # Compute AUROC\n",
    "    auroc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    # Compute Precision-Recall curve and AUPRC\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_scores)\n",
    "    auprc = auc(recall_curve, precision_curve)\n",
    "\n",
    "    return {\n",
    "        \"Threshold\": threshold,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall (Sensitivity)\": recall,\n",
    "        \"F1-score\": f1,\n",
    "        \"Specificity\": specificity,\n",
    "        \"NPV\": npv,\n",
    "        \"AUROC\": auroc,\n",
    "        \"AUPRC\": auprc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5021bad-2fe9-44aa-af65-1706fa564c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_best_threshold(predictionfile, outcome_col=\"outcome\", pred_col=\"Prediction\", thresholds=np.arange(0.00, 1.0, 0.05)):\n",
    "    \"\"\"\n",
    "    Find the threshold that maximizes the F1-score.\n",
    "\n",
    "    Parameters:\n",
    "    - predictionfile: Path to the CSV file containing predictions.\n",
    "    - outcome_col: Column name for the ground truth binary labels.\n",
    "    - pred_col: Column name for the predicted probabilities.\n",
    "    - thresholds: List or array of threshold values to test.\n",
    "\n",
    "    Returns:\n",
    "    - Best threshold and corresponding max F1-score.\n",
    "    \"\"\"\n",
    "    best_threshold = None\n",
    "    best_f1 = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        metrics = evaluate_threshold(predictionfile, outcome_col, pred_col, threshold)\n",
    "        f1 = metrics[\"F1-score\"]\n",
    "        # print(f\"Threshold: {threshold:.2f}, F1-score: {f1}, Accuracy: {metrics['Accuracy']}, Precision: {metrics['Precision']}, Recall: {metrics['Recall (Sensitivity)']}, Specificity: {metrics['Specificity']}, NPV: {metrics['NPV']}, AUROC: {metrics['AUROC']}, AUPRC: {metrics['AUPRC']}\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc595a-e067-4ad1-a4a0-1fa7216dbf4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\"Model\", \"Best Threshold\", \"Best F1-score\"])\n",
    "\n",
    "Nlist = [20, 40, 60, 80, 100] \n",
    "num_unfrozen_layers = [0, 1, 3, 6, 7, 8, 9, 11, 12, 15, 16, 18]\n",
    "\n",
    "for N in Nlist: \n",
    "    for num_unfrozen_layer in num_unfrozen_layers:\n",
    "        filename = f\"out/final_val_predictions_AoUencoder_StanfordFinetune-{num_unfrozen_layer}-{N}pct.csv\"\n",
    "        best_threshold, best_f1 = find_best_threshold(filename)\n",
    "        \n",
    "        result = pd.DataFrame({\n",
    "            \"Model\": [filename],\n",
    "            \"Best Threshold\": [best_threshold],\n",
    "            \"Best F1-score\": [best_f1]\n",
    "        })\n",
    "        \n",
    "        results_df = pd.concat([results_df, result], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97267a-245c-4e67-b2f8-a6a320741090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7e6a6-996e-45ef-88a0-c03deebcc1be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df.to_csv(\"thresholds_f1s_1_29.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098307b7-bbfd-4e40-bc56-be0a31fe872a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_value_row = results_df.loc[results_df['Best F1-score'].idxmax()]\n",
    "\n",
    "print(max_value_row)\n",
    "print(max_value_row['Model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dcaf5d-1d0a-4c52-8f13-de74da23264c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After finding the best thresholds for each model on the validation set, get test metrics\n",
    "results_df['Test Model'] = results_df['Model'].str.replace('final_val_predictions', 'final_test_predictions')\n",
    "\n",
    "test_metrics = pd.concat([\n",
    "    evaluate_predictions(row['Test Model'], pred_threshold=row['Best Threshold']).assign(Model=row['Test Model'])\n",
    "    for _, row in results_df.iterrows()\n",
    "])\n",
    "\n",
    "# Display the combined results\n",
    "print(test_metrics.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a049a5d-d9a8-4503-a112-3147dd97f7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_metrics.to_csv(\"test_metrics_1_20.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
